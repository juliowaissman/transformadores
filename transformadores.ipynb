{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformadores.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZ4r2A0uzvs0hfKNcTTiOf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliowaissman/transformadores/blob/main/transformadores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZmf9TidlZqP"
      },
      "source": [
        "![](https://huggingface.co/front/assets/huggingface_logo.svg)\n",
        "\n",
        "# Transformadores y modelos modernos de PLN\n",
        "\n",
        "## Curso de Procesamiento de Lenguaje Natural\n",
        "\n",
        "### Maestría en Ciencia de Datos\n",
        "\n",
        "### Universidad de Sonora\n",
        "\n",
        "**Olivia Gutú** y **Julio Waissman** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veLBiSxQl8TU"
      },
      "source": [
        "Los *transformadores* son modelos de redes neuronales que están diseñados para manejar datos secuenciales, como el lenguaje natural. A diferencia de los RNN, los transformadores utilizan un modelo que se conoce como *mecanismo de atención* que no requiere que los datos secuenciales se procesen en orden. \n",
        "\n",
        "Por ejemplo, si los datos de entrada son una oración en lenguaje natural, el transformador no necesita procesar el principio antes del final. Debido a esta característica, el transformador permite mucha más paralelización que los RNN y, por lo tanto, reduce los tiempos de entrenamiento.\n",
        "\n",
        "Estos modelos se están utilizando muy intensivamente para abordar muchos problemas de PNL, reemplazando los modelos de redes neuronales recurrentes más antiguos, como los LSTM. Dado que estos modelos facilitan una mayor paralelización durante el entrenamiento, ha permitido el entrenamiento en conjuntos de datos más grandes de lo que era posible antes de su introducción.\n",
        "\n",
        "Una buena explicación de lo que son los transformadores, y la idea subyaciente del mecanismo de atención se puede consultar en [este artículo de Medium](https://towardsdatascience.com/transformers-141e32e69591)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqzX6wob2fSl"
      },
      "source": [
        "En esta libreta vamos a explorar el uso de la librería `transformers` de la empresa [Hugging Face](https://huggingface.co). Esta librería es un *hub* que permite cargar y aplicar diferentes modelos preentrenados de transformadores para diferentes tareas. Igualmente es posible realizar un entrenamiento fino de los modelos a casos particulares, y someter dichos modelos para que puedan ser utilizados por otros.\n",
        "\n",
        "Para instalar `transformers` es necesario contar con *TensorFlow 2.x* y/o *pyTorch*. En general la mayoría de los modelos vienen en *pyTorch*. Si lo ejecutas desde *Colab*, el entorno ya viene con ambas librerias tensoriales preinstaladas.\n",
        "\n",
        "Par algunos modelos es necesario contar con [el módulo `sentencepiece`](https://github.com/google/sentencepiece) que es un módulo desarrollado por google para tokenizar, basado solo en la estructura de unigramas (independiente del lenguaje). Viene con funciones para acelerar tokenizadores existentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx46Lg6t3z5p"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hPWuAbT5OAz"
      },
      "source": [
        "La librería viene con un comando llamado `pipeline` que permite seleccionar un modelo preentrenado y aplicarlo a una serie de tareas (si el modelo viene adaptado a la tarea). La interfase es realmente sencilla de utilizar y si tenemos un modelo preentrenado que sea suficiente es una manera fácil de utilizar modelos modernos en aplicaciones.\n",
        "\n",
        "Las tareas para las que se puede utilizar el comando `pipeline`son:\n",
        "\n",
        "- `\"feature-extraction\"`\n",
        "- `\"sentiment-analysis\"`\n",
        "- `\"ner\"`\n",
        "- `\"question-answering\"`\n",
        "- `\"fill-mask\"`\n",
        "- `\"summarization\"`\n",
        "- `\"translation_xx_to_yy\"`\n",
        "- `\"text2text-generation\"`\n",
        "- `\"text-generation\"`\n",
        "- `\"zero-shot-classification:`\n",
        "- `\"conversational\"`\n",
        "\n",
        "Vamos a explorar la aplicación de alguna de ellas en esta libreta. Para mayor información del uso de `pipeline`puedes consultar [la documentación correspondiente](https://huggingface.co/transformers/task_summary.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QKnECvk3m1J"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Si quieres ver la documentación de pipeline solo descomenta esta linea\n",
        "#help(pipeline)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmQ5BP4X9Vw7"
      },
      "source": [
        "Para usar el `pipeline` es necesario seleccionar un modelo preentrenado. En la documentación se muestra [una serie de modelos generales y sus características](https://huggingface.co/transformers/model_summary.html). Lo mejor es ir y consultar la [lista exhaustiva de modelos preentrenados](https://huggingface.co/models) de *Hugging Face*.\n",
        "\n",
        "Vamos a practicar con algunas tareas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtE0VHg5-ayA"
      },
      "source": [
        "## Contestando preguntas\n",
        "\n",
        "Para ejemplificar esta tarea vamos a descargar el texto de una conferencia Matutina del Presidente de México del [proyecto público de NOSTRODATA](https://github.com/NOSTRODATA/conferencias_matutinas_amlo), y nos vamos a quedar con el discurso inicial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2yft2WD_lZg"
      },
      "source": [
        "import pandas as pd\n",
        "import pprint\n",
        "\n",
        "file = 'https://raw.githubusercontent.com/NOSTRODATA/conferencias_matutinas_amlo/master/2021/4-2021/abril%2014%2C%202021/mananera_14_04_2021.csv'\n",
        "df = pd.read_csv(file)\n",
        "    \n",
        "ind = min(df.index[df.Participante != df.Participante[0]]) - 1\n",
        "contexto = \"\\n\".join(df.Texto[:ind])\n",
        "\n",
        "pprint.pprint(contexto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQOjj1Q__q-Z"
      },
      "source": [
        "Ahora vamos autilizar un modelo tipo [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf) entrenado en español para responder preguntas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1_U6NHU_s70"
      },
      "source": [
        "nlp = pipeline(\n",
        "    'question-answering', \n",
        "    model='mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',\n",
        "    tokenizer=(\n",
        "        'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',  \n",
        "        {\"use_fast\": False}\n",
        "    )\n",
        ")\n",
        "\n",
        "preguntas = [\n",
        "  '¿Qué compañia se constituyó?',\n",
        "  '¿Cual es la audiencia potencial?',\n",
        "  '¿Que les pidió?'\n",
        "] \n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = nlp({'question': pregunta, 'context': contexto})\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"Pregunta: {pregunta}\")\n",
        "    print(f\"Respuesta: \\\"{respuesta['answer']}\\\", con un score de {respuesta['score']}\")\n",
        "print(\"=\" * 30) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_IbLGxTCsg5"
      },
      "source": [
        "**Prueba ahora con otro texto (otra mañanera, o una entrada de Wikipedia por ejemplo) y revis el resultado.**\n",
        "\n",
        "**¿Es el único modelo para responer preguntas en un contexto en español entre los modelos de Hugging Faces? Si hay otro(s) anotalos aqui mismo.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpPPR8LTDHII"
      },
      "source": [
        "## Haciendo resumenes de un texto\n",
        "\n",
        "¿Y como podríamos resumir el documento que acabamos de ver? Vamos a probar un modelo llamad [mT5](https://arxiv.org/pdf/2010.11934.pdf) para hacer resumenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9QKshLmDTwx"
      },
      "source": [
        "resumidor = pipeline(\n",
        "    \"summarization\", \n",
        "    model=\"LeoCordoba/mt5-small-mlsum\"\n",
        ")\n",
        "resumen = resumidor(contexto, min_length=5, max_length=500)\n",
        "print(\"\\n\\nResumen:\\n\\n\" + resumen[0]['summary_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV7sceuYIgoI"
      },
      "source": [
        "**Prueba cambiando los parámetros que puedes modificar (`min_lenght`y `max_lenght`). Revisa el resumen de otro texto (como una descripción de un sitio turístico por ejemplo).**\n",
        "\n",
        "**¿Qué otros modelos existen para hacer resúmenes de textos en español?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKOAxduUDzDx"
      },
      "source": [
        "## Clasificación *zero-shot*\n",
        "\n",
        "La clasificación *zero shot* es un método semi-supervisado, en el cual a un documento se le da una serie de opciones, para la cual no estuvo previamente entrenado el sistema, pero en función del contexto (vectores de palabras de nuevo) el modelo trata de inferir las categorias más probables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjMyE1YnJD1K"
      },
      "source": [
        "classificador = pipeline(\n",
        "    \"zero-shot-classification\", \n",
        "    model=\"Recognai/bert-base-spanish-wwm-cased-xnli\"\n",
        ")\n",
        "\n",
        "textos = [\n",
        "    \"El autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo\",\n",
        "    \"Se realizó la fusión de Televisa con Univisión y se constituyó la compañía de medios en español más grande del mundo.\",\n",
        "    \"El Real Madrid de nuevo se encuentra disputando la semifinal de la Copa de Campeones de la UEFA\",\n",
        "    \"La proxima semana inicia la vacunación contra COVID para el personal de educación\",\n",
        "    \"Aglomeraciones y falta de medidas se presentan en los mítines y eventos de las campañas de todos los candidatos\"\n",
        "]\n",
        "etiquetas = [\"cultura\", \"sociedad\", \"economia\", \"politica\", \"salud\", \"deportes\"]\n",
        "\n",
        "for texto in textos:\n",
        "    resultado = classificador(\n",
        "        texto, \n",
        "        candidate_labels=etiquetas, \n",
        "        hypothesis_template=\"Este texto es sobre {}.\"\n",
        "    )\n",
        "    print(\"=\" * 80)\n",
        "    print(result['sequence'])\n",
        "    for label, score in zip(result['labels'], result['scores']):\n",
        "        print(f\"\\t\\t{label} ({score})\")\n",
        "print(\"=\" * 80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds3OH-hLKkP8"
      },
      "source": [
        "**Prueba cambiando categorías, el template de la hipótesis y los textos, para entender como funciona el método**\n",
        "\n",
        "**¿exiten otros modelos preentrenados para hacer clasificación *zero-shot*? Si es el caso, enuncia uno.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws_kpFsJLEHa"
      },
      "source": [
        "## RuPERTa \n",
        "\n",
        "Hasta ahorita pareciera cosa mágica, pero es importante estar pendientes que no todos los modelos basados en transformadores son buenos. Algunos se han entrenado con realmente muy pocos casos y solo funcionan correctamente en casos muy estandar. \n",
        "\n",
        "Para ilustrar eso, así de como se toma un modelo que sirve para varios propósitos, pero con malos resultados, vamos autilizar el modelo *RuPERTa* que es el modelo [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) entrenado en un corpus grande en español.\n",
        "\n",
        "El modelo es muy útil y se aplica en diferentes tareas con éxito, pero en algunas apñicaciones su desempeño deja mucho que desear (sobre todo si no se utiliza un manejo del lenguaje similar al que se hace en España). Tenemos que entrenar un modelo *mexicano*.\n",
        "\n",
        "Empecemos por hacer POS-Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBNl34E-NCnS"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "\n",
        "pos2label = {\n",
        "    \"0\": \"O\",\n",
        "    \"1\": \"ADJ\",\n",
        "    \"2\": \"ADP\",\n",
        "    \"3\": \"ADV\",\n",
        "    \"4\": \"AUX\",\n",
        "    \"5\": \"CCONJ\",\n",
        "    \"6\": \"DET\",\n",
        "    \"7\": \"INTJ\",\n",
        "    \"8\": \"NOUN\",\n",
        "    \"9\": \"NUM\",\n",
        "    \"10\": \"PART\",\n",
        "    \"11\": \"PRON\",\n",
        "    \"12\": \"PROPN\",\n",
        "    \"13\": \"PUNCT\",\n",
        "    \"14\": \"SCONJ\",\n",
        "    \"15\": \"SYM\",\n",
        "    \"16\": \"VERB\"\n",
        "}\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('mrm8488/RuPERTa-base-finetuned-pos')\n",
        "model = AutoModelForTokenClassification.from_pretrained('mrm8488/RuPERTa-base-finetuned-pos')\n",
        "\n",
        "text = (\n",
        "    \"Julio Waissman, profesor de la Universidad de Sonora, \" +\n",
        "    \"nació en Cuauhtemoc, Chihuahua \" +\n",
        "    \"y está pensando en viajar a Puebla en vacaciones.\"\n",
        ")\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "outputs = model(input_ids)\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "for m in last_hidden_states:\n",
        "  for index, n in enumerate(m):\n",
        "    if(index > 0 and index <= len(text.split(\" \"))):\n",
        "      print(text.split(\" \")[index-1] + \": \" + pos2label[str(torch.argmax(n).item())])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da3CUqlqPIv2"
      },
      "source": [
        "Bastante, bastante mal ¿verdad? Ahora veamos como funciona el NER:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRQrURhGPO0R"
      },
      "source": [
        "ner2label = {\n",
        "    \"0\": \"B-LOC\",\n",
        "    \"1\": \"B-MISC\",\n",
        "    \"2\": \"B-ORG\",\n",
        "    \"3\": \"B-PER\",\n",
        "    \"4\": \"I-LOC\",\n",
        "    \"5\": \"I-MISC\",\n",
        "    \"6\": \"I-ORG\",\n",
        "    \"7\": \"I-PER\",\n",
        "    \"8\": \"O\"\n",
        "}\n",
        "tokenizer = AutoTokenizer.from_pretrained('mrm8488/RuPERTa-base-finetuned-ner')\n",
        "model = AutoModelForTokenClassification.from_pretrained('mrm8488/RuPERTa-base-finetuned-ner')\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "outputs = model(input_ids)\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "for m in last_hidden_states:\n",
        "  for index, n in enumerate(m):\n",
        "    if(index > 0 and index <= len(text.split(\" \"))):\n",
        "      print(text.split(\" \")[index-1] + \": \" + ner2label[str(torch.argmax(n).item())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iftgA6amQXnu"
      },
      "source": [
        "Y para terminar de desencantarnos de RuPERTa vamos a terminar usando el modelo para llenar espacios vacios:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VQOISHnQWc5"
      },
      "source": [
        "pipeline_fill_mask = pipeline(\n",
        "    \"fill-mask\", \n",
        "    model='mrm8488/RuPERTa-base'\n",
        ")\n",
        "\n",
        "rellena = pipeline_fill_mask(\"México es un país muy <mask> en el mundo\")\n",
        "\n",
        "print(\"\\n\\n\\n\\nMéxico es un país muy <mask> en Latinoamérica\\n\")\n",
        "print(\"=\" * 30)\n",
        "for caso in rellena:\n",
        "    print(f\"{caso['sequence']} ({caso['score']})\")\n",
        "print(\"=\" * 30)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBazHQQBQz0_"
      },
      "source": [
        "**¿No hay otros modelos para llenar espacio vacío en español con resultados decentes? Revisa si hay otro y aplicalo a ver si da mejores resultados.**\n",
        "\n",
        "**¿Porqué haríamos el NER o el POS tagging con transformadores, teniendo el modelo preentrenado de SpaCy?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpRwev2CRWU8"
      },
      "source": [
        "## Generación de texto\n",
        "\n",
        "La generación de texto tuvo un [hito importante con el modelo GPT-2](https://openai.com/blog/better-language-models/), tanto que el modelo *GPT-3* lo mantuvieron privado por la capacidad que tenía para generar texto falso creible, y su potencial aplicación en el desarrollo de bots de noticias falsas.\n",
        "\n",
        "Para terminar con los transformadores, vamos a ver un modelo sencillo en español para generación de texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xstsctWSY31"
      },
      "source": [
        "generador = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"datificate/gpt2-small-spanish\"\n",
        ")\n",
        "\n",
        "texto_generado = generador(\n",
        "    \"Se encontraron sapos alucinógenos en la Isla Tiburon\", \n",
        "    max_length=100,\n",
        ")\n",
        "for muestra in texto_generado:\n",
        "  pprint.pprint(muestra['generated_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDvcCqwtVij_"
      },
      "source": [
        "texto_generado = generador(\n",
        "    \"¿Quén es Pito Pérez?\", \n",
        "    max_length=100,\n",
        ")\n",
        "for muestra in texto_generado:\n",
        "  pprint.pprint(muestra['generated_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvE0l0ymWPe0"
      },
      "source": [
        "**¿Hay otros modelos mejor entrenados en español para la generación de texto automático? Revisa diferentes modelos**\n",
        "\n",
        "**Con el modelo encontrado ¿Se podrían generar bots maliciosos para esparcir *fakes news* en tuiter?**"
      ]
    }
  ]
}